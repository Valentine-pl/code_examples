import sys
import boto3
import json
import logging
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions
from delta.tables import DeltaTable
from pyspark.sql.functions import col, lit, expr, to_timestamp, to_date, year, month, dayofmonth, when
import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def initialize_glue_context_and_job():
    """Initialize AWS Glue context and job parameters."""
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_BUCKET', 'S3_KEY', 'DB_URL', 'DB_USER', 'DB_PASSWORD'])
    sc = SparkContext.getOrCreate()
    glue_context = GlueContext(sc)
    spark = glue_context.spark_session
    spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "LEGACY")
    job = Job(glue_context)
    job.init(args['JOB_NAME'], args)
    return args, glue_context, spark, job

def download_s3_json(bucket, key):
    """Download a JSON file from S3 and return its content."""
    s3 = boto3.client('s3')
    try:
        obj = s3.get_object(Bucket=bucket, Key=key)
        json_content = json.loads(obj['Body'].read().decode('utf-8'))
        return json_content
    except Exception as e:
        logger.error(f"Error downloading JSON from S3: {e}")
        raise
        
def generate_sql_query_for_table(table_config, default_datetime='1990-01-01 00:00:00'):
    table_name = table_config['table_name']
    updated_at_col = table_config['updated_at_col']
    created_at_col = table_config['created_at_col']
    updated_at_hour_col = table_config.get('updated_at_hour_col')
    created_at_hour_col = table_config.get('created_at_hour_col')
    load_from_date = table_config.get('load_from_date')
    last_updated_date = table_config.get('last_updated_date', '1990-01-01 00:00:00')
    
    effective_date = load_from_date if load_from_date else last_updated_date

    if updated_at_hour_col and created_at_hour_col:
        # Logic for tables with separate date and time columns
        query = f"""
        SELECT 
            CASE 
                WHEN {updated_at_hour_col} IS NOT NULL AND TO_CHAR(COALESCE({updated_at_col}, {created_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')), 'HH24:MI:SS') = '00:00:00'
                THEN TO_TIMESTAMP(TO_CHAR(COALESCE({updated_at_col}, {created_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')), 'DD-MM-YYYY') || ' ' || {updated_at_hour_col}, 'DD-MM-YYYY HH24:MI:SS')
                ELSE COALESCE({updated_at_col}, {created_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS'))
            END AS corrected_updated_at,
            CASE 
                WHEN {created_at_hour_col} IS NOT NULL AND TO_CHAR(COALESCE({created_at_col}, {updated_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')), 'HH24:MI:SS') = '00:00:00'
                THEN TO_TIMESTAMP(TO_CHAR(COALESCE({created_at_col}, {updated_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')), 'DD-MM-YYYY') || ' ' || {created_at_hour_col}, 'DD-MM-YYYY HH24:MI:SS')
                ELSE COALESCE({created_at_col}, {updated_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS'))
            END AS corrected_created_at,
            t.*
        FROM {table_name} t
        WHERE COALESCE({updated_at_col}, {created_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')) > TO_TIMESTAMP('{effective_date}', 'YYYY-MM-DD HH24:MI:SS')
        """
    else:
        # Logic for tables without separate time columns
        query = f"""
        SELECT 
            COALESCE({updated_at_col}, {created_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')) AS corrected_updated_at,
            COALESCE({created_at_col}, {updated_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')) AS corrected_created_at,
            t.*
        FROM {table_name} t
        WHERE COALESCE({updated_at_col}, {created_at_col}, TO_TIMESTAMP('{default_datetime}', 'YYYY-MM-DD HH24:MI:SS')) > TO_TIMESTAMP('{effective_date}', 'YYYY-MM-DD HH24:MI:SS')
        """
    return query.strip()

def load_data_from_source(spark, table_config, db_url, db_user, db_password):
    """Load data from the Oracle database using a custom SQL query."""
    custom_query = generate_sql_query_for_table(table_config, "1990-01-01 00:00:00") 
    df = spark.read.format("jdbc") \
        .option("url", db_url) \
        .option("query", custom_query) \
        .option("user", db_user) \
        .option("password", db_password) \
        .load()
    return df.distinct()

def transform_data(df):
    """Transform the data according to the configuration."""
    df = df.select([col(c).alias(c.lower()) for c in df.columns]) 
    df = df.withColumn("year", year(col("corrected_updated_at")))
    df = df.withColumn("month", month(col("corrected_updated_at")))
    df = df.withColumn("day", dayofmonth(col("corrected_updated_at")))
    return df.repartition("year", "month")

def write_data_to_delta_table(spark, df, table_config):
    """Write the processed data to a Delta table."""
    delta_table_path = table_config["delta_table_path"]
    if DeltaTable.isDeltaTable(spark, delta_table_path):
        delta_table = DeltaTable.forPath(spark, delta_table_path)
        merge_data_into_delta_table(delta_table, df, table_config)
    else:
        df.write.format("delta").partitionBy("year", "month").option("path", delta_table_path).mode("overwrite").save()

def merge_data_into_delta_table(delta_table, df, table_config):
    """Merge processed data into an existing Delta table."""
    composite_key_cols = table_config["composite_key_cols"]
    merge_condition = " AND ".join([f"existing.{col} = new_data.{col}" for col in composite_key_cols])
    update_set = {col_name: col("new_data." + col_name) for col_name in df.columns if col_name not in composite_key_cols}
    delta_table.alias("existing").merge(
        df.alias("new_data"), merge_condition
    ).whenMatchedUpdate(set=update_set).execute()

def update_last_updated_date(table_config, df):
    """Update the last_updated_date in the table configuration based on the dataframe."""
    max_date_result = df.agg({"corrected_updated_at": "max"}).collect()[0][0]
    if max_date_result:  # Ensure max_date is not None
        max_date = max_date_result.strftime('%Y-%m-%d %H:%M:%S')
        table_config['last_updated_date'] = max_date
    return table_config
    
def get_db_credentials(secret_id, region_name):
    """Retrieve database credentials from AWS Secrets Manager."""
    try:
        client = boto3.client('secretsmanager')
        get_secret_value_response = client.get_secret_value(SecretId=secret_id)
        secret = get_secret_value_response['SecretString']
        print("Successfully retrieved secret:", secret)  # For debugging; consider logging instead
        return json.loads(secret)
    except Exception as e:
        print(f"Failed to retrieve secret: {e}")  # Consider logging this instead
        return None    
  
def process_table(glue_context, spark, table_config):
    """Process a single table configuration."""
    secret_id = 'glue/ora'
    region_name = 'eu-west-1'
    credentials = get_db_credentials(secret_id, region_name)
    db_url = f"jdbc:oracle:thin:@//{credentials['host']}:{credentials['port']}/{credentials['dbname']}"
    db_user = credentials['username']
    db_password = credentials['password']
    df = load_data_from_source(spark, table_config, db_url, db_user, db_password)
    df_transformed = transform_data(df)
    write_data_to_delta_table(spark, df_transformed, table_config)
    
    # After successful data load and write, update the last_updated_date
    updated_table_config = update_last_updated_date(table_config, df_transformed)
    
    # If load_from_date was used, remove it for subsequent runs
    if 'load_from_date' in updated_table_config:
        del updated_table_config['load_from_date'] 
        
    return updated_table_config

def write_updated_config_to_s3(bucket, key, updated_config):
    """Write the updated tables configuration to S3."""
    s3 = boto3.client('s3')
    s3.put_object(Bucket=bucket, Key=key, Body=json.dumps(updated_config, indent=4).encode('utf-8'))

if __name__ == "__main__":
    args, glue_context, spark, job = initialize_glue_context_and_job()
    tables_config = download_s3_json(args['S3_BUCKET'], args['S3_KEY'])
    updated_tables_config = []
    for table_config in tables_config:
        updated_table_config = process_table(glue_context, spark, table_config)
        updated_tables_config.append(updated_table_config)
    write_updated_config_to_s3(args['S3_BUCKET'], args['S3_KEY'], updated_tables_config)
    job.commit()
